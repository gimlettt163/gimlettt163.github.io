<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="根据已有的分词器训练新的分词器 训练标记器与训练模型不同！模型训练使用随机梯度下降使每个batch的loss小一点。它本质上是随机的（这意味着在进行两次相同的训练时，您必须设置一些随机数种子才能获得相同的结果）。训练标记器是一个统计过程，它试图确定哪些子词最适合为给定的语料库选择，用于选择它们的确切规则取决于分词算法。它是确定性的，这意味着在相同的语料库上使用相同的算法进行训练时，您总是会得到相同">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace学习记录-第六章">
<meta property="og:url" content="http://example.com/2024/04/24/HuggingFace%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-%E7%AC%AC%E5%85%AD%E7%AB%A0/index.html">
<meta property="og:site_name" content="Gimlettt&#39;s Blog">
<meta property="og:description" content="根据已有的分词器训练新的分词器 训练标记器与训练模型不同！模型训练使用随机梯度下降使每个batch的loss小一点。它本质上是随机的（这意味着在进行两次相同的训练时，您必须设置一些随机数种子才能获得相同的结果）。训练标记器是一个统计过程，它试图确定哪些子词最适合为给定的语料库选择，用于选择它们的确切规则取决于分词算法。它是确定性的，这意味着在相同的语料库上使用相同的算法进行训练时，您总是会得到相同">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="c:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240425111516288.png">
<meta property="og:image" content="c:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240425111455251.png">
<meta property="og:image" content="c:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240425111531329.png">
<meta property="og:image" content="c:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240425115528978.png">
<meta property="og:image" content="c:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240428124703577.png">
<meta property="article:published_time" content="2024-04-24T13:09:14.000Z">
<meta property="article:modified_time" content="2024-06-04T09:22:50.061Z">
<meta property="article:author" content="Gimlettt163">
<meta property="article:tag" content="HuggingFace学习　">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240425111516288.png">

<link rel="canonical" href="http://example.com/2024/04/24/HuggingFace%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-%E7%AC%AC%E5%85%AD%E7%AB%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>HuggingFace学习记录-第六章 | Gimlettt's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Gimlettt's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/24/HuggingFace%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-%E7%AC%AC%E5%85%AD%E7%AB%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Gimlettt163">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gimlettt's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          HuggingFace学习记录-第六章
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-04-24 21:09:14" itemprop="dateCreated datePublished" datetime="2024-04-24T21:09:14+08:00">2024-04-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-06-04 17:22:50" itemprop="dateModified" datetime="2024-06-04T17:22:50+08:00">2024-06-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="根据已有的分词器训练新的分词器"><a href="#根据已有的分词器训练新的分词器" class="headerlink" title="根据已有的分词器训练新的分词器"></a>根据已有的分词器训练新的分词器</h1><blockquote>
<p>训练标记器与训练模型不同！模型训练使用随机梯度下降使每个batch的loss小一点。它本质上是随机的（这意味着在进行两次相同的训练时，您必须设置一些随机数种子才能获得相同的结果）。训练标记器是一个统计过程，它试图确定哪些子词最适合为给定的语料库选择，用于选择它们的确切规则取决于分词算法。它是确定性的，这意味着在相同的语料库上使用相同的算法进行训练时，您总是会得到相同的结果。</p>
</blockquote>
<p>本节内容是训练新分词器的一个简单示例。</p>
<p>先用了一个训练好的tokenizer：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">old_tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>然后用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, <span class="number">52000</span>)</span><br></pre></td></tr></table></figure>

<p>创建了一个新的tokenizer。注意train_new_from_iterator(training_corpus, 52000)：</p>
<blockquote>
<p>Transformers 中有一个非常简单的 API，你可以用它来训练一个新的标记器，使它与现有标记器相同的特征： <strong>AutoTokenizer.train_new_from_iterator()</strong> .</p>
</blockquote>
<p>然后就是用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokens = tokenizer.tokenize(example)</span><br><span class="line">tokens</span><br></pre></td></tr></table></figure>

<h1 id="快速标记器的特殊能力"><a href="#快速标记器的特殊能力" class="headerlink" title="快速标记器的特殊能力"></a>快速标记器的特殊能力</h1><p>这一节和之前看的命名体识别有点相关的（不幸找不到视频了）。总之就是机器通过打标签的方式标记实体属于哪一类别（主语、宾语）以及哪些字符属于同一个实体，例如一个人的名字有三个汉字组成，机器需要把这三个汉字标记为同一个人，因此需要用到begin、end之类的标记。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line">example = <span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span></span><br><span class="line">encoding = tokenizer(example)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(encoding))</span><br></pre></td></tr></table></figure>

<p><img src="C:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240425111516288.png" alt="image-20240425111516288"></p>
<p><img src="C:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240425111455251.png" alt="image-20240425111455251"></p>
<img src="C:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240425111531329.png" alt="image-20240425111531329" style="zoom:50%;" />

<blockquote>
<p>最后，我们可以将任何单词或标记映射到原始文本中的字符，反之亦然，通过 <strong>word_to_chars()</strong> 或者 <strong>token_to_chars()</strong> 和 <strong>char_to_word()</strong> 或者 <strong>char_to_token()</strong> 方法。例如， <strong>word_ids()</strong> 方法告诉我们 <strong>##yl</strong> 是索引 3 处单词的一部分，但它是句子中的哪个单词？</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start, end = encoding.word_to_chars(<span class="number">3</span>)</span><br><span class="line">example[start:end]</span><br><span class="line">Output:Sylvain</span><br></pre></td></tr></table></figure>

<p>使用pipeline分类的结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">token_classifier = pipeline(<span class="string">&quot;token-classification&quot;</span>)</span><br><span class="line">token_classifier(<span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)</span><br><span class="line">Output:<span class="comment">#注意start和end</span></span><br><span class="line">    [&#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9993828</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;S&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">12</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99815476</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##yl&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">14</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99590725</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##va&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">14</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">16</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9992327</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##in&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">18</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.97389334</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Hu&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">35</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.976115</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">13</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##gging&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">35</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">40</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.98879766</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">14</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Face&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">41</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">45</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-LOC&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99321055</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Brooklyn&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">49</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">57</span>&#125;]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">token_classifier = pipeline(<span class="string">&quot;token-classification&quot;</span>, aggregation_strategy=<span class="string">&quot;simple&quot;</span>)</span><br><span class="line">token_classifier(<span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)</span><br><span class="line">Output:</span><br><span class="line">    [&#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9981694</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Sylvain&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">18</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.97960204</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Hugging Face&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">45</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;LOC&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99321055</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Brooklyn&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">49</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">57</span>&#125;]</span><br></pre></td></tr></table></figure>

<p>后面是关于怎么把同一个实体合并的内容，暂不记录。</p>
<h1 id="QA-管道中的快速标记器"><a href="#QA-管道中的快速标记器" class="headerlink" title="QA 管道中的快速标记器"></a>QA 管道中的快速标记器</h1><p>略</p>
<h1 id="标准化和预标记化"><a href="#标准化和预标记化" class="headerlink" title="标准化和预标记化"></a>标准化和预标记化</h1><blockquote>
<p>标准化步骤涉及一些常规清理，例如删除不必要的空格、小写和&#x2F;或删除重音符号。Transformers <strong>tokenizer</strong> 有一个属性叫做 <strong>backend_tokenizer</strong> 它提供了对 Tokenizers 库中底层标记器的访问：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tokenizer.backend_tokenizer))</span><br><span class="line">Output:</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;tokenizers.Tokenizer&#x27;</span>&gt;</span><br><span class="line"><span class="comment">#normalizer 的属性 tokenizer 对象有一个 normalize_str() 我们可以用来查看标准化是如何执行的方法：</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer.backend_tokenizer.normalizer.normalize_str(<span class="string">&quot;Héllò hôw are ü?&quot;</span>))</span><br><span class="line">Output:</span><br><span class="line"><span class="string">&#x27;hello how are u?&#x27;</span><span class="comment">#在这个例子中，因为我们选择了 bert-base-uncased 检查点，标准化应用小写并删除重音。</span></span><br></pre></td></tr></table></figure>

<h2 id="预标记化"><a href="#预标记化" class="headerlink" title="预标记化"></a>预标记化</h2><blockquote>
<p>分词器不能单独在原始文本上进行训练。需要将文本拆分为小实体，例如单词。这就是预标记化步骤的用武之地。 基于单词的标记器可以简单地将原始文本拆分为空白和标点符号的单词。这些词将是分词器在训练期间可以学习的子标记的边界。要查看快速分词器如何执行预分词，我们可以使用 <strong>pre_tokenize_str()</strong> 的方法 <strong>pre_tokenizer</strong> 的属性 <strong>tokenizer</strong> 目的：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Hello, how are  you?&quot;</span>)</span><br><span class="line">Output:</span><br><span class="line">    [(<span class="string">&#x27;Hello&#x27;</span>, (<span class="number">0</span>, <span class="number">5</span>)), (<span class="string">&#x27;,&#x27;</span>, (<span class="number">5</span>, <span class="number">6</span>)), (<span class="string">&#x27;how&#x27;</span>, (<span class="number">7</span>, <span class="number">10</span>)), (<span class="string">&#x27;are&#x27;</span>, (<span class="number">11</span>, <span class="number">14</span>)), (<span class="string">&#x27;you&#x27;</span>, (<span class="number">16</span>, <span class="number">19</span>)), (<span class="string">&#x27;?&#x27;</span>, (<span class="number">19</span>, <span class="number">20</span>))]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Hello, how are  you?&quot;</span>)</span><br><span class="line">Output:</span><br><span class="line">    [(<span class="string">&#x27;Hello&#x27;</span>, (<span class="number">0</span>, <span class="number">5</span>)), (<span class="string">&#x27;,&#x27;</span>, (<span class="number">5</span>, <span class="number">6</span>)), (<span class="string">&#x27;Ġhow&#x27;</span>, (<span class="number">6</span>, <span class="number">10</span>)), (<span class="string">&#x27;Ġare&#x27;</span>, (<span class="number">10</span>, <span class="number">14</span>)), (<span class="string">&#x27;Ġ&#x27;</span>, (<span class="number">14</span>, <span class="number">15</span>)), (<span class="string">&#x27;Ġyou&#x27;</span>, (<span class="number">15</span>, <span class="number">19</span>)),</span><br><span class="line"> (<span class="string">&#x27;?&#x27;</span>, (<span class="number">19</span>, <span class="number">20</span>))]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;t5-small&quot;</span>)</span><br><span class="line"><span class="comment">#基于 SentencePiece 算法的 T5 分词器</span></span><br><span class="line">tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Hello, how are  you?&quot;</span>)</span><br><span class="line">Output:</span><br><span class="line">    [(<span class="string">&#x27;▁Hello,&#x27;</span>, (<span class="number">0</span>, <span class="number">6</span>)), (<span class="string">&#x27;▁how&#x27;</span>, (<span class="number">7</span>, <span class="number">10</span>)), (<span class="string">&#x27;▁are&#x27;</span>, (<span class="number">11</span>, <span class="number">14</span>)), (<span class="string">&#x27;▁you?&#x27;</span>, (<span class="number">16</span>, <span class="number">20</span>))]</span><br></pre></td></tr></table></figure>

<p>gpt2也会在空格和标点符号上拆分，但它会保留空格并将它们替换为 <strong>Ġ</strong> 符号，如果我们解码令牌，则使其能够恢复原始空格;与 BERT 分词器不同，此分词器不会忽略双空格。</p>
<p>与 GPT-2 标记器一样，T5标记器保留空格并用特定标记替换它们（ <strong>_</strong> )，但 T5 分词器只在空格上拆分，而不是标点符号。还要注意，它默认在句子的开头添加了一个空格（之前 <strong>Hello</strong> ) 并忽略了之间的双空格 <strong>are</strong> 和 <strong>you</strong> .</p>
<h2 id="句子"><a href="#句子" class="headerlink" title="句子"></a>句子</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">SentencePiece</a> 是一种用于文本预处理的标记化算法，您可以将其与我们将在接下来的三个部分中看到的任何模型一起使用。它将文本视为 Unicode 字符序列，并用特殊字符替换空格， <strong>▁</strong> .与 Unigram 算法结合使用（参见<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter7/7">section 7</a>), 它甚至不需要预标记化步骤，这对于不使用空格字符的语言（如中文或日语）非常有用。</p>
<p>SentencePiece 的另一个主要特点是可逆标记化：由于没有对空格进行特殊处理，因此只需通过将它们连接起来并替换 <strong>_</strong> s 带空格——这会导致标准化的文本。正如我们之前看到的，BERT 分词器删除了重复的空格，因此它的分词是不可逆的。</p>
</blockquote>
<img src="C:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240425115528978.png" alt="image-20240425115528978" style="zoom:50%;" />

<h1 id="字节对编码标记化BPE"><a href="#字节对编码标记化BPE" class="headerlink" title="字节对编码标记化BPE"></a>字节对编码标记化BPE</h1><blockquote>
<p>字节对编码(BPE)最初被开发为一种压缩文本的算法,然后在预训练 GPT 模型时被 OpenAI 用于标记化。许多 Transformer 模型都使用它,包括 GPT、GPT-2、RoBERTa、BART 和 DeBERTa。</p>
</blockquote>
<h2 id="标记与合并"><a href="#标记与合并" class="headerlink" title="标记与合并"></a>标记与合并</h2><p>如何统计组合的频率？<br>首先将字符（单词）拆分为最小的字母。</p>
<p>假设文本库里有如下单词，具有如下频率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;pug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;pun&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;bun&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hugs&quot;</span>, <span class="number">5</span>)</span><br><span class="line">可以写成标记列表：</span><br><span class="line">(<span class="string">&quot;h&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>然后我们看成对。这对 <code>(&quot;h&quot;, &quot;u&quot;)</code> 出现在单词 <code>&quot;hug&quot;</code> 和 <code>&quot;hugs&quot;</code>中,所以语料库中总共有15次。不过,这并不是最频繁的一对:这个荣誉属于 <code>(&quot;u&quot;, &quot;g&quot;)</code>,它出现在 <code>&quot;hug&quot;</code>, <code>&quot;pug&quot;</code>, 以及 <code>&quot;hugs&quot;</code>中,在词汇表中总共 20 次。</p>
<p>因此,标记器学习的第一个合并规则是 <code>(&quot;u&quot;, &quot;g&quot;) -&gt; &quot;ug&quot;</code>,意思就是 <code>&quot;ug&quot;</code> 将被添加到词汇表中,并且这对应该合并到语料库的所有单词中。在这个阶段结束时,词汇表和语料库看起来像这样:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary: [<span class="string">&quot;b&quot;</span>, <span class="string">&quot;g&quot;</span>, <span class="string">&quot;h&quot;</span>, <span class="string">&quot;n&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;u&quot;</span>, <span class="string">&quot;ug&quot;</span>]（单词表中增加了组合“ug”）</span><br><span class="line">Corpus: (<span class="string">&quot;h&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;ug&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p>接着以此类推，又学习了“un”、“hug”组合，直到达到所需的词汇量。</p>
<h2 id="标记化算法"><a href="#标记化算法" class="headerlink" title="标记化算法"></a>标记化算法</h2><blockquote>
<p>标记化紧跟训练过程,从某种意义上说,通过应用以下步骤对新输入进行标记:</p>
<ol>
<li>规范化</li>
<li>预标记化</li>
<li>将单词拆分为单个字符</li>
<li>将学习的合并规则按顺序应用于这些拆分</li>
</ol>
</blockquote>
<p>根据前文生成的词汇表， <code>&quot;bug&quot;</code> 将被标记为 <code>[&quot;b&quot;, &quot;ug&quot;]</code>。 <code>&quot;mug&quot;</code>,将被标记为 <code>[&quot;[UNK]&quot;, &quot;ug&quot;]</code>，因为字母 <code>&quot;m&quot;</code> 不在基本词汇表中。同样,单词<code>&quot;thug&quot;</code> 会被标记为 <code>[&quot;[UNK]&quot;, &quot;hug&quot;]</code>：字母 <code>&quot;t&quot;</code> 不在基本词汇表中，应用合并规则首先导致 <code>&quot;u&quot;</code> 和 <code>&quot;g&quot;</code> 被合并,然后是 <code>&quot;hu&quot;</code> 和 <code>&quot;g&quot;</code> 被合并。</p>
<h2 id="实现-BPE"><a href="#实现-BPE" class="headerlink" title="实现 BPE"></a>实现 BPE</h2><p>假设有如下语料库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    <span class="string">&quot;This is the Hugging Face Course.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This chapter is about tokenization.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This section shows several tokenizer algorithms.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>使用分词器将该语料标记为单词，本例使用gpt-2。并计算每个单词的频率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">word_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus:</span><br><span class="line">    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    new_words = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> words_with_offsets]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> new_words:</span><br><span class="line">        word_freqs[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(word_freqs)</span><br><span class="line"></span><br><span class="line"><span class="comment">#结果为</span></span><br><span class="line">defaultdict(<span class="built_in">int</span>, &#123;<span class="string">&#x27;This&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;Ġis&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;Ġthe&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;ĠHugging&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;ĠFace&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;ĠCourse&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;Ġchapter&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;Ġabout&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġtokenization&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġsection&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġshows&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġseveral&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġtokenizer&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġalgorithms&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;Hopefully&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;,&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġyou&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġwill&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġbe&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġable&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġto&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġunderstand&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġhow&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;Ġthey&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġare&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġtrained&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġand&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġgenerate&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġtokens&#x27;</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure>

<p>下面计算语料中出现的所有字符：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">alphabet = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys():</span><br><span class="line">    <span class="keyword">for</span> letter <span class="keyword">in</span> word:</span><br><span class="line">        <span class="keyword">if</span> letter <span class="keyword">not</span> <span class="keyword">in</span> alphabet:</span><br><span class="line">            alphabet.append(letter)</span><br><span class="line">alphabet.sort()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(alphabet)</span><br><span class="line"><span class="comment">#并加入gpt2的特殊标记：</span></span><br><span class="line">vocab = [<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>] + alphabet.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment">#将每个单词拆分为单独的字符：</span></span><br><span class="line">splits = &#123;word: [c <span class="keyword">for</span> c <span class="keyword">in</span> word] <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys()&#125;</span><br></pre></td></tr></table></figure>

<p>编写每对字母出现的频率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_pair_freqs</span>(<span class="params">splits</span>):</span><br><span class="line">    pair_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> word_freqs.items():<span class="comment">#word_freqs为出现的单词及频率，不包括标点符号</span></span><br><span class="line">        split = splits[word]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(split) - <span class="number">1</span>):</span><br><span class="line">            pair = (split[i], split[i + <span class="number">1</span>])</span><br><span class="line">            pair_freqs[pair] += freq</span><br><span class="line">    <span class="keyword">return</span> pair_freqs</span><br></pre></td></tr></table></figure>

<p>找到最频率的对，并将其添加到vocab：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">best_pair = <span class="string">&quot;&quot;</span></span><br><span class="line">max_freq = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pair, freq <span class="keyword">in</span> pair_freqs.items():</span><br><span class="line">    <span class="keyword">if</span> max_freq <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> max_freq &lt; freq:</span><br><span class="line">        best_pair = pair</span><br><span class="line">        max_freq = freq</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(best_pair, max_freq)</span><br><span class="line">Output:(<span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;t&#x27;</span>) <span class="number">7</span></span><br><span class="line">merges = &#123;(<span class="string">&quot;Ġ&quot;</span>, <span class="string">&quot;t&quot;</span>): <span class="string">&quot;Ġt&quot;</span>&#125;</span><br><span class="line">vocab.append(<span class="string">&quot;Ġt&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>继续接下来的步骤,我们需要在我们的<code>分词</code>字典中应用该合并:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">merge_pair</span>(<span class="params">a, b, splits</span>):</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs:</span><br><span class="line">        split = splits[word]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(split) - <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> split[i] == a <span class="keyword">and</span> split[i + <span class="number">1</span>] == b:</span><br><span class="line">                split = split[:i] + [a + b] + split[i + <span class="number">2</span> :]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        splits[word] = split</span><br><span class="line">    <span class="keyword">return</span> splits</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一次合并的结果</span></span><br><span class="line">splits = merge_pair(<span class="string">&quot;Ġ&quot;</span>, <span class="string">&quot;t&quot;</span>, splits)</span><br><span class="line"><span class="built_in">print</span>(splits[<span class="string">&quot;Ġtrained&quot;</span>])</span><br></pre></td></tr></table></figure>

<p>现在需要学会想要的所有合并，目标是词汇量达到50：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(vocab) &lt; vocab_size:</span><br><span class="line">    pair_freqs = compute_pair_freqs(splits)</span><br><span class="line">    best_pair = <span class="string">&quot;&quot;</span></span><br><span class="line">    max_freq = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> pair, freq <span class="keyword">in</span> pair_freqs.items():</span><br><span class="line">        <span class="keyword">if</span> max_freq <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> max_freq &lt; freq:</span><br><span class="line">            best_pair = pair</span><br><span class="line">            max_freq = freq</span><br><span class="line">    splits = merge_pair(*best_pair, splits)</span><br><span class="line">    merges[best_pair] = best_pair[<span class="number">0</span>] + best_pair[<span class="number">1</span>]</span><br><span class="line">    vocab.append(best_pair[<span class="number">0</span>] + best_pair[<span class="number">1</span>])</span><br><span class="line"><span class="comment">#词汇表由特殊标记、初始字母和所有合并结果组成</span></span><br></pre></td></tr></table></figure>

<p>进行预分词、拆分以对新文本进行分词，应用学到的所有合并规则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text</span>):</span><br><span class="line">    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    pre_tokenized_text = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> pre_tokenize_result]</span><br><span class="line">    splits = [[l <span class="keyword">for</span> l <span class="keyword">in</span> word] <span class="keyword">for</span> word <span class="keyword">in</span> pre_tokenized_text]</span><br><span class="line">    <span class="keyword">for</span> pair, merge <span class="keyword">in</span> merges.items():</span><br><span class="line">        <span class="keyword">for</span> idx, split <span class="keyword">in</span> <span class="built_in">enumerate</span>(splits):</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(split) - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> split[i] == pair[<span class="number">0</span>] <span class="keyword">and</span> split[i + <span class="number">1</span>] == pair[<span class="number">1</span>]:</span><br><span class="line">                    split = split[:i] + [merge] + split[i + <span class="number">2</span> :]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            splits[idx] = split</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(splits, [])</span><br><span class="line">tokenize(<span class="string">&quot;This is not a token.&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Word-Piece"><a href="#Word-Piece" class="headerlink" title="Word Piece"></a>Word Piece</h1><h2 id="标记化算法-1"><a href="#标记化算法-1" class="headerlink" title="标记化算法"></a>标记化算法</h2><p>与 BPE 一样，Word Piece 从一个小词汇表开始，包括模型使用的特殊标记和初始字母表。因为它通过添加前缀来识别子词 ，每个单词最初是通过将该前缀添加到单词内的所有字符来拆分的，例如 <code>&quot;word&quot;</code> ，像这样拆分：w ##o ##r ##d。</p>
<p>然后，再次像 BPE 一样，Word Piece 学习合并规则。主要区别在于选择要合并的对的方式。Word Piece 不是选择最频繁的对，而是使用以下公式计算每对的分数：<br>$$<br>score&#x3D;(freq_of_pair)&#x2F;(freq_of_first_element×freq_of_second_element)<br>$$<br>通过将<code>配对的频率</code>除以<code>其每个部分的频率的乘积</code>, 该算法优先合并<code>单个部分在词汇表中频率较低的对</code>。例如，它不一定会合并 <code>(&quot;un&quot;, &quot;##able&quot;)</code> 即使这对在词汇表中出现的频率很高，因为 <code>&quot;un&quot;</code> 和 <code>&quot;##able&quot;</code> 很可能每个词都出现在很多其他词中并且出现频率很高。相比之下，像 <code>(&quot;hu&quot;, &quot;##gging&quot;)</code> 可能会更快地合并 (假设 “hugging” 经常出现在词汇表中),因为 <code>&quot;hu&quot;</code> 和 <code>&quot;##gging&quot;</code> 这两个词<code>单独出现的频率</code>可能较低。</p>
<p>以BPE相同的语料库为例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)</span><br><span class="line">拆分为:</span><br><span class="line">(&quot;h&quot; &quot;##u&quot; &quot;##g&quot;, 10), (&quot;p&quot; &quot;##u&quot; &quot;##g&quot;, 5), (&quot;p&quot; &quot;##u&quot; &quot;##n&quot;, 12), (&quot;b&quot; &quot;##u&quot; &quot;##n&quot;, 4), (&quot;h&quot; &quot;##u&quot; &quot;##g&quot; &quot;##s&quot;, 5)</span><br></pre></td></tr></table></figure>

<p>最频繁的一对是 <code>(&quot;##u&quot;, &quot;##g&quot;)</code> (目前20次)，但 <code>&quot;##u&quot;</code> 单独出现的频率非常高，所以它的分数不是最高的(它是 1 &#x2F; 36)。所有带有 <code>&quot;##u&quot;</code> 的对实际上都有相同的分数(1 &#x2F; 36)，所以分数最高的对是 <code>(&quot;##g&quot;, &quot;##s&quot;)</code> — 唯一没有 <code>&quot;##u&quot;</code> 的对— 1 &#x2F; 20,所以学习的第一个合并是 <code>(&quot;##g&quot;, &quot;##s&quot;) -&gt; (&quot;##gs&quot;)</code>。</p>
<p>合并时删除了两个标记之间的 <code>##</code>，所以添加 <code>&quot;##gs&quot;</code> 到词汇表中，并在语料库的单词中应用该合并：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary: [&quot;b&quot;, &quot;h&quot;, &quot;p&quot;, &quot;##g&quot;, &quot;##n&quot;, &quot;##s&quot;, &quot;##u&quot;, &quot;##gs&quot;]</span><br><span class="line">Corpus: (&quot;h&quot; &quot;##u&quot; &quot;##g&quot;, 10), (&quot;p&quot; &quot;##u&quot; &quot;##g&quot;, 5), (&quot;p&quot; &quot;##u&quot; &quot;##n&quot;, 12), (&quot;b&quot; &quot;##u&quot; &quot;##n&quot;, 4), (&quot;h&quot; &quot;##u&quot; &quot;##gs&quot;, 5)</span><br></pre></td></tr></table></figure>

<p>以此类推，合并hu、hug，直到达到指定词汇量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary: [&quot;b&quot;, &quot;h&quot;, &quot;p&quot;, &quot;##g&quot;, &quot;##n&quot;, &quot;##s&quot;, &quot;##u&quot;, &quot;##gs&quot;, &quot;hu&quot;, &quot;hug&quot;]</span><br><span class="line">Corpus: (&quot;hug&quot;, 10), (&quot;p&quot; &quot;##u&quot; &quot;##g&quot;, 5), (&quot;p&quot; &quot;##u&quot; &quot;##n&quot;, 12), (&quot;b&quot; &quot;##u&quot; &quot;##n&quot;, 4), (&quot;hu&quot; &quot;##gs&quot;, 5)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Word Piece 和 BPE 中的标记化的不同在于 Word Piece 只保存最终词汇,而不是学习的合并规则。从要标记的单词开始,Word Piece 找到词汇表中最长的子词,然后对其进行拆分。例如,如果我们使用上面例子中学到的词汇,对于单词 <code>&quot;hugs&quot;</code>,词汇表中从头开始的最长子词是 <code>&quot;hug&quot;</code>,所以我们在那里拆分并得到 <code>[&quot;hug&quot;, &quot;##s&quot;]</code>。 然后我们继续使用词汇表中的 <code>&quot;##s&quot;</code>,因此 <code>&quot;hugs&quot;</code> 的标记化是 <code>[&quot;hug&quot;, &quot;##s&quot;]</code>.</p>
<p>使用 BPE, 我们将按顺序应用学习到的合并并将其标记为 <code>[&quot;hu&quot;, &quot;##gs&quot;]</code>,所以编码不同。</p>
<p>再举一个例子,让我们看看 <code>&quot;bugs&quot;</code> 将如何被标记化。 <code>&quot;b&quot;</code> 是从词汇表中单词开头开始的最长子词,所以我们在那里拆分并得到 <code>[&quot;b&quot;, &quot;##ugs&quot;]</code>。然后 <code>&quot;##u&quot;</code> 是词汇表中从 <code>&quot;##ugs&quot;</code> 开始的最长的子词,所以我们在那里拆分并得到 <code>[&quot;b&quot;, &quot;##u, &quot;##gs&quot;]</code>。最后, <code>&quot;##gs&quot;</code> 在词汇表中,所以最后一个列表是 <code>&quot;bugs&quot;</code> 的标记化。</p>
<p>当分词达到无法在词汇表中找到子词的阶段时, 整个词被标记为未知 — 例如, <code>&quot;mug&quot;</code> 将被标记为 <code>[&quot;[UNK]&quot;]</code>,就像 <code>&quot;bum&quot;</code> (即使我们可以以 <code>&quot;b&quot;</code> 和 <code>&quot;##u&quot;</code> 开始, <code>&quot;##m&quot;</code> 不在词汇表中,由此产生的标记将只是 <code>[&quot;[UNK]&quot;]</code>, 不是 <code>[&quot;b&quot;, &quot;##u&quot;, &quot;[UNK]&quot;]</code>)。这是与 BPE 的另一个区别,BPE 只会将不在词汇表中的单个字符分类为未知。</p>
</blockquote>
<h2 id="实现-Word-Piece"><a href="#实现-Word-Piece" class="headerlink" title="实现 Word Piece"></a>实现 Word Piece</h2><p>和BPE同样的语料库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    <span class="string">&quot;This is the Hugging Face Course.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This chapter is about tokenization.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This section shows several tokenizer algorithms.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>首先将语料库预先标记为单词。使用 <code>bert-base-cased</code> 标记器用于预标记化，并在进行预标记化时计算语料库中每个单词的频率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"><span class="comment">#在进行预标记化时计算语料库中每个单词的频率</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">word_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus:</span><br><span class="line">    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    new_words = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> words_with_offsets]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> new_words:</span><br><span class="line">        word_freqs[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">word_freqs</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">    defaultdict(</span><br><span class="line">    <span class="built_in">int</span>, &#123;<span class="string">&#x27;This&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;is&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;the&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Hugging&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Face&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Course&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;chapter&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;about&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;tokenization&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;section&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;shows&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;several&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;tokenizer&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;algorithms&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Hopefully&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;,&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;you&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;will&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;be&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;able&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;to&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;understand&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;how&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;they&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;are&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;trained&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;and&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;generate&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;tokens&#x27;</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure>

<p>字母表是由单词的<code>所有第一个字母组成的唯一集合</code>,以及出现在前缀为 <code>##</code> 的其他字母:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">alphabet = []</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys():</span><br><span class="line">    <span class="keyword">if</span> word[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> alphabet:</span><br><span class="line">        alphabet.append(word[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> letter <span class="keyword">in</span> word[<span class="number">1</span>:]:</span><br><span class="line">        <span class="keyword">if</span> <span class="string">f&quot;##<span class="subst">&#123;letter&#125;</span>&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> alphabet:</span><br><span class="line">            alphabet.append(<span class="string">f&quot;##<span class="subst">&#123;letter&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">alphabet.sort()</span><br><span class="line">alphabet</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(alphabet)</span><br><span class="line">Output:</span><br><span class="line">    [<span class="string">&#x27;##a&#x27;</span>, <span class="string">&#x27;##b&#x27;</span>, <span class="string">&#x27;##c&#x27;</span>, <span class="string">&#x27;##d&#x27;</span>, <span class="string">&#x27;##e&#x27;</span>, <span class="string">&#x27;##f&#x27;</span>, <span class="string">&#x27;##g&#x27;</span>, <span class="string">&#x27;##h&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##k&#x27;</span>, <span class="string">&#x27;##l&#x27;</span>, <span class="string">&#x27;##m&#x27;</span>, <span class="string">&#x27;##n&#x27;</span>, <span class="string">&#x27;##o&#x27;</span>, <span class="string">&#x27;##p&#x27;</span>, <span class="string">&#x27;##r&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;##t&#x27;</span>, <span class="string">&#x27;##u&#x27;</span>, <span class="string">&#x27;##v&#x27;</span>, <span class="string">&#x27;##w&#x27;</span>, <span class="string">&#x27;##y&#x27;</span>, <span class="string">&#x27;##z&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;F&#x27;</span>, <span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;u&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;y&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>还在该词汇表的开头添加了模型使用的特殊标记，在使用 BERT 的情况下,它是列表 <code>[&quot;[PAD]&quot;, &quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[MASK]&quot;]</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocab = [<span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>] + alphabet.copy()</span><br></pre></td></tr></table></figure>

<p>拆分每个单词，所有不是第一个字母的字母都以 <code>##</code> 为前缀：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">splits = &#123;</span><br><span class="line">    word: [c <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="string">f&quot;##<span class="subst">&#123;c&#125;</span>&quot;</span> <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(word)]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>接下来编写一个函数来计算每对的分数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_pair_scores</span>(<span class="params">splits</span>):</span><br><span class="line">    letter_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    pair_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> word_freqs.items():</span><br><span class="line">        split = splits[word]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split) == <span class="number">1</span>:</span><br><span class="line">            letter_freqs[split[<span class="number">0</span>]] += freq</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(split) - <span class="number">1</span>):</span><br><span class="line">            pair = (split[i], split[i + <span class="number">1</span>])</span><br><span class="line">            letter_freqs[split[i]] += freq</span><br><span class="line">            pair_freqs[pair] += freq</span><br><span class="line">        letter_freqs[split[-<span class="number">1</span>]] += freq</span><br><span class="line"></span><br><span class="line">    scores = &#123;</span><br><span class="line">        pair: freq / (letter_freqs[pair[<span class="number">0</span>]] * letter_freqs[pair[<span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">for</span> pair, freq <span class="keyword">in</span> pair_freqs.items()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> scores</span><br><span class="line"><span class="comment">#字典在初始拆分后的一部分:</span></span><br><span class="line">pair_scores = compute_pair_scores(splits)</span><br><span class="line"><span class="keyword">for</span> i, key <span class="keyword">in</span> <span class="built_in">enumerate</span>(pair_scores.keys()):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>: <span class="subst">&#123;pair_scores[key]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> i &gt;= <span class="number">5</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">Output:</span><br><span class="line">(<span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;##h&#x27;</span>): <span class="number">0.125</span></span><br><span class="line">(<span class="string">&#x27;##h&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>): <span class="number">0.03409090909090909</span></span><br><span class="line">(<span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>): <span class="number">0.02727272727272727</span></span><br><span class="line">(<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>): <span class="number">0.1</span></span><br><span class="line">(<span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;##h&#x27;</span>): <span class="number">0.03571428571428571</span></span><br><span class="line">(<span class="string">&#x27;##h&#x27;</span>, <span class="string">&#x27;##e&#x27;</span>): <span class="number">0.011904761904761904</span></span><br><span class="line"><span class="comment">#快速循环找到得分最高的对：</span></span><br><span class="line">best_pair = <span class="string">&quot;&quot;</span></span><br><span class="line">max_score = <span class="literal">None</span></span><br><span class="line"><span class="keyword">for</span> pair, score <span class="keyword">in</span> pair_scores.items():</span><br><span class="line">    <span class="keyword">if</span> max_score <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> max_score &lt; score:</span><br><span class="line">        best_pair = pair</span><br><span class="line">        max_score = score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(best_pair, max_score)</span><br><span class="line">Output:</span><br><span class="line">(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;##b&#x27;</span>) <span class="number">0.2</span>   </span><br><span class="line"><span class="comment">#所以第一个要学习的合并是 `(&#x27;a&#x27;, &#x27;##b&#x27;) -&gt; &#x27;ab&#x27;`。我们添加 `&#x27;ab&#x27;` 到词汇表中:</span></span><br><span class="line">vocab.append(<span class="string">&quot;ab&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>继续接下来的步骤，需要在 <code>拆分</code> 的字典中应用该合并：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">merge_pair</span>(<span class="params">a, b, splits</span>):</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs:</span><br><span class="line">        split = splits[word]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(split) - <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> split[i] == a <span class="keyword">and</span> split[i + <span class="number">1</span>] == b:</span><br><span class="line">                merge = a + b[<span class="number">2</span>:] <span class="keyword">if</span> b.startswith(<span class="string">&quot;##&quot;</span>) <span class="keyword">else</span> a + b</span><br><span class="line">                split = split[:i] + [merge] + split[i + <span class="number">2</span> :]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        splits[word] = split</span><br><span class="line">    <span class="keyword">return</span> splits</span><br><span class="line"><span class="comment">#第一次合并的结果：</span></span><br><span class="line">splits = merge_pair(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;##b&quot;</span>, splits)</span><br><span class="line">splits[<span class="string">&quot;about&quot;</span>]</span><br><span class="line">Output:</span><br><span class="line">[<span class="string">&#x27;ab&#x27;</span>, <span class="string">&#x27;##o&#x27;</span>, <span class="string">&#x27;##u&#x27;</span>, <span class="string">&#x27;##t&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>假设目标词汇量为70，过程为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = <span class="number">70</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(vocab) &lt; vocab_size:</span><br><span class="line">    scores = compute_pair_scores(splits)</span><br><span class="line">    best_pair, max_score = <span class="string">&quot;&quot;</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> pair, score <span class="keyword">in</span> scores.items():</span><br><span class="line">        <span class="keyword">if</span> max_score <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> max_score &lt; score:</span><br><span class="line">            best_pair = pair</span><br><span class="line">            max_score = score</span><br><span class="line">    splits = merge_pair(*best_pair, splits)</span><br><span class="line">    new_token = (</span><br><span class="line">        best_pair[<span class="number">0</span>] + best_pair[<span class="number">1</span>][<span class="number">2</span>:]</span><br><span class="line">        <span class="keyword">if</span> best_pair[<span class="number">1</span>].startswith(<span class="string">&quot;##&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span> best_pair[<span class="number">0</span>] + best_pair[<span class="number">1</span>]</span><br><span class="line">    )</span><br><span class="line">    vocab.append(new_token)</span><br><span class="line"><span class="comment">#查看生成的词汇表</span></span><br><span class="line"><span class="built_in">print</span>(vocab)</span><br><span class="line">[<span class="string">&#x27;[PAD]&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>, <span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>, <span class="string">&#x27;[MASK]&#x27;</span>, <span class="string">&#x27;##a&#x27;</span>, <span class="string">&#x27;##b&#x27;</span>, <span class="string">&#x27;##c&#x27;</span>, <span class="string">&#x27;##d&#x27;</span>, <span class="string">&#x27;##e&#x27;</span>, <span class="string">&#x27;##f&#x27;</span>, <span class="string">&#x27;##g&#x27;</span>, <span class="string">&#x27;##h&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##k&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;##l&#x27;</span>, <span class="string">&#x27;##m&#x27;</span>, <span class="string">&#x27;##n&#x27;</span>, <span class="string">&#x27;##o&#x27;</span>, <span class="string">&#x27;##p&#x27;</span>, <span class="string">&#x27;##r&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>, <span class="string">&#x27;##t&#x27;</span>, <span class="string">&#x27;##u&#x27;</span>, <span class="string">&#x27;##v&#x27;</span>, <span class="string">&#x27;##w&#x27;</span>, <span class="string">&#x27;##y&#x27;</span>, <span class="string">&#x27;##z&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;F&#x27;</span>, <span class="string">&#x27;H&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;ab&#x27;</span>,<span class="string">&#x27;##fu&#x27;</span>, <span class="string">&#x27;Fa&#x27;</span>, <span class="string">&#x27;Fac&#x27;</span>, <span class="string">&#x27;##ct&#x27;</span>, <span class="string">&#x27;##ful&#x27;</span>, <span class="string">&#x27;##full&#x27;</span>, <span class="string">&#x27;##fully&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;Th&#x27;</span>, <span class="string">&#x27;ch&#x27;</span>, <span class="string">&#x27;##hm&#x27;</span>, <span class="string">&#x27;cha&#x27;</span>, <span class="string">&#x27;chap&#x27;</span>, <span class="string">&#x27;chapt&#x27;</span>, <span class="string">&#x27;##thm&#x27;</span>, <span class="string">&#x27;Hu&#x27;</span>, <span class="string">&#x27;Hug&#x27;</span>, <span class="string">&#x27;Hugg&#x27;</span>, <span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;th&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;##thms&#x27;</span>, <span class="string">&#x27;##za&#x27;</span>, <span class="string">&#x27;##zat&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;##ut&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>为了对新文本进行分词，我们对其进行预分词、拆分，然后对每个单词应用分词算法。也就是说，从第一个词的开头寻找最大的子词并将其拆分，然后我们在第二部分重复这个过程，对于该词的其余部分和文本中的以下词，依此类推：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">encode_word</span>(<span class="params">word</span>):</span><br><span class="line">    tokens = []</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(word) &gt; <span class="number">0</span>:</span><br><span class="line">        i = <span class="built_in">len</span>(word)</span><br><span class="line">        <span class="keyword">while</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> word[:i] <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            i -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="string">&quot;[UNK]&quot;</span>]</span><br><span class="line">        tokens.append(word[:i])</span><br><span class="line">        word = word[i:]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(word) &gt; <span class="number">0</span>:</span><br><span class="line">            word = <span class="string">f&quot;##<span class="subst">&#123;word&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">return</span> tokens</span><br><span class="line"><span class="built_in">print</span>(encode_word(<span class="string">&quot;Hugging&quot;</span>))</span><br><span class="line">[<span class="string">&#x27;Hugg&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##n&#x27;</span>, <span class="string">&#x27;##g&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(encode_word(<span class="string">&quot;HOgging&quot;</span>))</span><br><span class="line">[<span class="string">&#x27;[UNK]&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>标记文本的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text</span>):</span><br><span class="line">    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    pre_tokenized_text = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> pre_tokenize_result]</span><br><span class="line">    encoded_words = [encode_word(word) <span class="keyword">for</span> word <span class="keyword">in</span> pre_tokenized_text]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(encoded_words, [])</span><br><span class="line">tokenize(<span class="string">&quot;This is the Hugging Face course!&quot;</span>)</span><br><span class="line">Output:</span><br><span class="line">[<span class="string">&#x27;Th&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;th&#x27;</span>, <span class="string">&#x27;##e&#x27;</span>, <span class="string">&#x27;Hugg&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##n&#x27;</span>, <span class="string">&#x27;##g&#x27;</span>, <span class="string">&#x27;Fac&#x27;</span>, <span class="string">&#x27;##e&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;##o&#x27;</span>, <span class="string">&#x27;##u&#x27;</span>, <span class="string">&#x27;##r&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;##e&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h1 id="Unigram"><a href="#Unigram" class="headerlink" title="Unigram"></a>Unigram</h1><blockquote>
<p>与 BPE 和 Word Piece 相比，Unigram 在另一个方向上工作：它从一个较大的词汇表开始,然后从中<code>删除</code>标记,直到达到所需的词汇表大小。有多种选项可用于构建基本词汇表：例如，我们可以采用预标记化单词中最常见的子串，或者在具有大词汇量的初始语料库上应用 BPE。</p>
<p>在训练的每一步，Unigram 算法都会在给定当前词汇的情况下计算语料库的损失。然后，对于词汇表中的每个符号，算法计算如果删除该符号，整体损失会增加多少，并寻找增加最少的符号。这些符号对语料库的整体损失影响较小，因此从某种意义上说，它们“不太需要”并且是移除的最佳候选者。</p>
<p>这是一个非常昂贵的操作，所以我们不只是删除与最低损失增加相关的单个符号，而且(p) ((p)是一个可以控制的超参数，通常是 10 或 20)与最低损失增加相关的符号的百分比。然后重复这个过程,直到词汇量达到所需的大小。</p>
<p>请注意，我们从不删除基本字符，以确保可以标记任何单词。</p>
<p>现在，这仍然有点模糊：算法的主要部分是计算语料库的损失，并查看当我们从词汇表中删除一些标记时它会如何变化，但我们还没有解释如何做到这一点。这一步依赖于 Unigram 模型的标记化算法，因此我们接下来将深入研究。</p>
</blockquote>
<p>继续使用相同的语料库，并拆分成子字符串：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)</span><br><span class="line">[&quot;h&quot;, &quot;u&quot;, &quot;g&quot;, &quot;hu&quot;, &quot;ug&quot;, &quot;p&quot;, &quot;pu&quot;, &quot;n&quot;, &quot;un&quot;, &quot;b&quot;, &quot;bu&quot;, &quot;s&quot;, &quot;hug&quot;, &quot;gs&quot;, &quot;ugs&quot;]</span><br></pre></td></tr></table></figure>

<h2 id="标记化算法-2"><a href="#标记化算法-2" class="headerlink" title="标记化算法"></a>标记化算法</h2><p>Unigram 认为每个标记都独立于它之前的标记，是最简单的语言模型。从某种意义上说， 给定先前上下文的标记 X 的概率就是标记 X 的概率。因此，使用 Unigram 语言模型生成文本将始终预测最常见的标记。</p>
<p>给定标记的概率是它在原始语料库中的频率(我们找到它的次数)，除以词汇表中所有标记的所有频率的总和(以确保概率总和为 1)。例如, <code>&quot;ug&quot;</code> 在 <code>&quot;hug&quot;</code> 、 <code>&quot;pug&quot;</code> 以及 <code>&quot;hugs&quot;</code> 中，所以它在我们的语料库中的频率为 20。以下是词汇表中所有可能的子词的出现频率:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&quot;h&quot;</span>, <span class="number">15</span>) (<span class="string">&quot;u&quot;</span>, <span class="number">36</span>) (<span class="string">&quot;g&quot;</span>, <span class="number">20</span>) (<span class="string">&quot;hu&quot;</span>, <span class="number">15</span>) (<span class="string">&quot;ug&quot;</span>, <span class="number">20</span>) (<span class="string">&quot;p&quot;</span>, <span class="number">17</span>) (<span class="string">&quot;pu&quot;</span>, <span class="number">17</span>) (<span class="string">&quot;n&quot;</span>, <span class="number">16</span>)</span><br><span class="line">(<span class="string">&quot;un&quot;</span>, <span class="number">16</span>) (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>) (<span class="string">&quot;bu&quot;</span>, <span class="number">4</span>) (<span class="string">&quot;s&quot;</span>, <span class="number">5</span>) (<span class="string">&quot;hug&quot;</span>, <span class="number">15</span>) (<span class="string">&quot;gs&quot;</span>, <span class="number">5</span>) (<span class="string">&quot;ugs&quot;</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment">#对上一步中所有的子字符串统计频率</span></span><br></pre></td></tr></table></figure>

<p>所有频率之和为210，因此子词 <code>&quot;ug&quot;</code> 出现的概率是 20&#x2F;210。</p>
<p>为了对给定的单词进行标记，将所有可能的分割视为标记，并根据 Unigram 模型计算每个分割的概率。由于所有标记都被认为是独立的，所以这个概率只是每个标记概率的乘积。例如, <code>&quot;pug&quot;</code> 的标记化 <code>[&quot;p&quot;, &quot;u&quot;, &quot;g&quot;]</code> 的概率为：<br>$$<br>P([‘p’,’u’,’g’])&#x3D;P(‘p’)×P(‘u’)×P(‘g’)&#x3D;<br>5&#x2F;210<br> ×<br>36&#x2F;210<br> ×<br>20&#x2F;210<br> &#x3D;0.000389<br>$$<br>标记化 <code>[&quot;pu&quot;, &quot;g&quot;]</code> 的概率为：<br>$$<br>𝑃([‘𝑝𝑢’,’𝑔’])&#x3D;𝑃(‘𝑝𝑢’)×𝑃(‘𝑔’)&#x3D;5&#x2F;210×20&#x2F;210&#x3D;0.0022676<br>$$<br>一般来说，具有尽可能少的标记的标记化将具有最高的概率(因为每个标记重复除以 210)。这对应于我们直观想要的：将一个单词分成尽可能少的标记。</p>
<p>使用 Unigram 模型对单词进行分词是概率最高的分词。在示例 <code>&quot;pug&quot;</code> 中,这里是我们为每个可能的分割获得的概率:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&quot;p&quot;</span>, <span class="string">&quot;u&quot;</span>, <span class="string">&quot;g&quot;</span>] : <span class="number">0.000389</span></span><br><span class="line">[<span class="string">&quot;p&quot;</span>, <span class="string">&quot;ug&quot;</span>] : <span class="number">0.0022676</span></span><br><span class="line">[<span class="string">&quot;pu&quot;</span>, <span class="string">&quot;g&quot;</span>] : <span class="number">0.0022676</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>所以, <code>&quot;pug&quot;</code> 将被标记为 <code>[&quot;p&quot;, &quot;ug&quot;]</code> 或者 <code>[&quot;pu&quot;, &quot;g&quot;]</code>, 取决于首先遇到这些分割中的哪一个(请注意,在更大的语料库中,这样的相等的情况很少见)。</p>
<p>在这种情况下,很容易找到所有可能的分割并计算它们的概率,但一般来说会有点困难。有一种用于此的经典算法,称为 <em>维特比(Viterbi)算法</em>。本质上,我们可以构建一个图来检测给定单词的可能分割,如果从<em>a</em>到<em>b</em>的子词在词汇表中,则从字符<em>a</em>到字符<em>b</em>之间存在一个分支,并将子词的概率归因于该分支。</p>
<p>为了在该图中找到将具有最佳分数的路径,维特比算法为单词中的每个位置确定在该位置结束的具有最佳分数的分段。由于我们从开始到结束,可以通过循环遍历以当前位置结尾的所有子词,然后使用该子词开始位置的最佳标记化分数来找到最佳分数。然后,我们只需要展开到达终点所采取的路径。</p>
</blockquote>
<p>使用我们的词汇表和单词 <code>&quot;unhug&quot;</code> 的例子。对于每个位置,以最好的分数结尾的子词如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Character 0 (u): &quot;u&quot; (score 0.171429)</span><br><span class="line">Character 1 (n): &quot;un&quot; (score 0.076191)</span><br><span class="line">Character 2 (h): &quot;un&quot; &quot;h&quot; (score 0.005442)</span><br><span class="line">Character 3 (u): &quot;un&quot; &quot;hu&quot; (score 0.005442)</span><br><span class="line">Character 4 (g): &quot;un&quot; &quot;hug&quot; (score 0.005442)</span><br></pre></td></tr></table></figure>

<p>因此 <code>&quot;unhug&quot;</code> 将被标记为 <code>[&quot;un&quot;, &quot;hug&quot;]</code>。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><blockquote>
<p>在任何给定的阶段,这个损失是通过对语料库中的每个单词进行标记来计算的,使用当前词汇表和由语料库中每个标记的频率确定的 Unigram 模型(如前所述)。语料库中的每个词都有一个分数,损失是这些分数的负对数似然 ：即所有词的语料库中所有词的总和 <code>log(P(word))</code>。</p>
</blockquote>
<p>在(“hug”, 10), (“pug”, 5), (“pun”, 12), (“bun”, 4), (“hugs”, 5)中，每个单词的标记化及其各自的分数是:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&quot;hug&quot;: [&quot;hug&quot;] (score 0.071428)</span><br><span class="line">&quot;pug&quot;: [&quot;pu&quot;, &quot;g&quot;] (score 0.007710)</span><br><span class="line">&quot;pun&quot;: [&quot;pu&quot;, &quot;n&quot;] (score 0.006168)</span><br><span class="line">&quot;bun&quot;: [&quot;bu&quot;, &quot;n&quot;] (score 0.001451)</span><br><span class="line">&quot;hugs&quot;: [&quot;hug&quot;, &quot;s&quot;] (score 0.001701)</span><br></pre></td></tr></table></figure>

<p>所以损失为：<br>$$<br>10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) &#x3D; 169.8<br>$$<br>现在计算删除每个标记如何影响损失。在这里只对两个标记进行操作，并保存整个过程以备有代码。在这里，对所有单词有两个等效的标记：例如 <code>&quot;pug&quot;</code> 可以以相同的分数被标记为 <code>[&quot;p&quot;, &quot;ug&quot;]</code>。因此去除词汇表中的 <code>&quot;pu&quot;</code> 标记将给出完全相同的损失；另一方面,去除 <code>&quot;hug&quot;</code> 损失变差 因为 <code>&quot;hug&quot;</code> 和 <code>&quot;hugs&quot;</code> 的标记化会变成:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;hug&quot;: [&quot;hu&quot;, &quot;g&quot;] (score 0.006802)</span><br><span class="line">&quot;hugs&quot;: [&quot;hu&quot;, &quot;gs&quot;] (score 0.001701)</span><br></pre></td></tr></table></figure>

<p>这些变化将导致损失增加:<br>$$</p>
<ul>
<li>10 * (-log(0.071428)) + 10 * (-log(0.006802)) &#x3D; 23.5<br>$$<br>因此，  <code>&quot;pu&quot;</code>可能会从词汇表中删除,但不会删除 <code>&quot;hug&quot;</code>。</li>
</ul>
<h2 id="实现-Unigram"><a href="#实现-Unigram" class="headerlink" title="实现 Unigram"></a>实现 Unigram</h2><p>语料库依然是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    <span class="string">&quot;This is the Hugging Face Course.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This chapter is about tokenization.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This section shows several tokenizer algorithms.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>使用 <code>xlnet-base-cased</code> 作为分词模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;xlnet-base-cased&quot;</span>)</span><br><span class="line"><span class="comment">#首先计算语料库中每个单词的出现次数：</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">word_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus:</span><br><span class="line">    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    new_words = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> words_with_offsets]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> new_words:</span><br><span class="line">        word_freqs[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">word_freqs</span><br><span class="line"><span class="comment">#然后需要将词汇表初始化大于最终的词汇量，因此必须包含所有基本字符(否则无法标记每个单词)。对于较大的子字符串只保留最常见的字符，按频率对它们进行排序:</span></span><br><span class="line">char_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">subwords_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> word, freq <span class="keyword">in</span> word_freqs.items():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word)):</span><br><span class="line">        char_freqs[word[i]] += freq</span><br><span class="line">        <span class="comment"># Loop through the subwords of length at least 2</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">2</span>, <span class="built_in">len</span>(word) + <span class="number">1</span>):</span><br><span class="line">            subwords_freqs[word[i:j]] += freq</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sort subwords by frequency</span></span><br><span class="line">sorted_subwords = <span class="built_in">sorted</span>(subwords_freqs.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">sorted_subwords[:<span class="number">10</span>]</span><br><span class="line">Output:</span><br><span class="line">[(<span class="string">&#x27;▁t&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;is&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;er&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;▁a&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;▁to&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;to&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;en&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;▁T&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;▁Th&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;▁Thi&#x27;</span>, <span class="number">3</span>)]</span><br><span class="line">token_freqs = <span class="built_in">list</span>(char_freqs.items()) + sorted_subwords[: <span class="number">300</span> - <span class="built_in">len</span>(char_freqs)]</span><br><span class="line">token_freqs = &#123;token: freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Sentence Piece 使用一种称为增强后缀数组(ESA)的更高效算法来创建初始词汇表。</p>
</blockquote>
<p>接下来计算所有频率的总和，并将频率转换为概率。将存储概率的对数，因为添加对数比乘以小数在数值上更稳定，可以简化模型损失的计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"></span><br><span class="line">total_sum = <span class="built_in">sum</span>([freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs.items()])</span><br><span class="line">model = &#123;token: -log(freq / total_sum) <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs.items()&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>N现在主要功能是使用 Viterbi 算法标记单词的功能。正如我们之前看到的,该算法计算单词的每个子串的最佳分段,我们将其存储在名为 <code>best_segmentations</code> 的变量中。我们将在单词的每个位置(从 0 到其总长度)存储一个字典,有两个键:最佳分割中最后一个标记的开始索引,以及最佳分割的分数。使用最后一个标记的开始索引,一旦列表完全填充,我们将能够检索完整的分段。</p>
<p>填充列表只需两个循环:主循环遍历每个起始位置,第二个循环尝试从该起始位置开始的所有子字符串。如果子串在词汇表中,我们有一个新的词分段,直到该结束位置,我们将其与 <code>best_segmentations</code> 相比较。</p>
</blockquote>
<p>一旦主循环完成就从结尾开始，从一个开始位置跳到下一个，记录前进的标记，直到到达单词的开头：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">encode_word</span>(<span class="params">word, model</span>):</span><br><span class="line">    best_segmentations = [&#123;<span class="string">&quot;start&quot;</span>: <span class="number">0</span>, <span class="string">&quot;score&quot;</span>: <span class="number">1</span>&#125;] + [</span><br><span class="line">        &#123;<span class="string">&quot;start&quot;</span>: <span class="literal">None</span>, <span class="string">&quot;score&quot;</span>: <span class="literal">None</span>&#125; <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word))</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">for</span> start_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word)):</span><br><span class="line">        <span class="comment"># This should be properly filled by the previous steps of the loop</span></span><br><span class="line">        best_score_at_start = best_segmentations[start_idx][<span class="string">&quot;score&quot;</span>]</span><br><span class="line">        <span class="keyword">for</span> end_idx <span class="keyword">in</span> <span class="built_in">range</span>(start_idx + <span class="number">1</span>, <span class="built_in">len</span>(word) + <span class="number">1</span>):</span><br><span class="line">            token = word[start_idx:end_idx]</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> model <span class="keyword">and</span> best_score_at_start <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                score = model[token] + best_score_at_start</span><br><span class="line">                <span class="comment"># If we have found a better segmentation ending at end_idx, we update</span></span><br><span class="line">                <span class="keyword">if</span> (</span><br><span class="line">                    best_segmentations[end_idx][<span class="string">&quot;score&quot;</span>] <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">                    <span class="keyword">or</span> best_segmentations[end_idx][<span class="string">&quot;score&quot;</span>] &gt; score</span><br><span class="line">                ):</span><br><span class="line">                    best_segmentations[end_idx] = &#123;<span class="string">&quot;start&quot;</span>: start_idx, <span class="string">&quot;score&quot;</span>: score&#125;</span><br><span class="line"></span><br><span class="line">    segmentation = best_segmentations[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> segmentation[<span class="string">&quot;score&quot;</span>] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># We did not find a tokenization of the word -&gt; unknown</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;&lt;unk&gt;&quot;</span>], <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    score = segmentation[<span class="string">&quot;score&quot;</span>]</span><br><span class="line">    start = segmentation[<span class="string">&quot;start&quot;</span>]</span><br><span class="line">    end = <span class="built_in">len</span>(word)</span><br><span class="line">    tokens = []</span><br><span class="line">    <span class="keyword">while</span> start != <span class="number">0</span>:</span><br><span class="line">        tokens.insert(<span class="number">0</span>, word[start:end])</span><br><span class="line">        next_start = best_segmentations[start][<span class="string">&quot;start&quot;</span>]</span><br><span class="line">        end = start</span><br><span class="line">        start = next_start</span><br><span class="line">    tokens.insert(<span class="number">0</span>, word[start:end])</span><br><span class="line">    <span class="keyword">return</span> tokens, score</span><br></pre></td></tr></table></figure>

<p>在一些词上尝试模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(encode_word(<span class="string">&quot;Hopefully&quot;</span>, model))</span><br><span class="line">([<span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;ll&#x27;</span>, <span class="string">&#x27;y&#x27;</span>], <span class="number">41.5157494601402</span>)</span><br><span class="line"><span class="built_in">print</span>(encode_word(<span class="string">&quot;This&quot;</span>, model))</span><br><span class="line">([<span class="string">&#x27;This&#x27;</span>], <span class="number">6.288267030694535</span>)</span><br></pre></td></tr></table></figure>

<p>计算语料库上的损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">model</span>):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> word_freqs.items():</span><br><span class="line">        _, word_loss = encode_word(word, model)</span><br><span class="line">        loss += freq * word_loss</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line">compute_loss(model)</span><br><span class="line">Output:<span class="number">413.10377642940875</span></span><br></pre></td></tr></table></figure>

<p>计算每个标记的分数只需要计算通过删除每个标记获得的模型的损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_scores</span>(<span class="params">model</span>):</span><br><span class="line">    scores = &#123;&#125;</span><br><span class="line">    model_loss = compute_loss(model)</span><br><span class="line">    <span class="keyword">for</span> token, score <span class="keyword">in</span> model.items():</span><br><span class="line">        <span class="comment"># We always keep tokens of length 1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(token) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        model_without_token = copy.deepcopy(model)</span><br><span class="line">        _ = model_without_token.pop(token)</span><br><span class="line">        scores[token] = compute_loss(model_without_token) - model_loss</span><br><span class="line">    <span class="keyword">return</span> scores</span><br><span class="line">scores = compute_scores(model)</span><br><span class="line"><span class="built_in">print</span>(scores[<span class="string">&quot;ll&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(scores[<span class="string">&quot;his&quot;</span>])</span><br><span class="line">Output:</span><br><span class="line"><span class="number">6.376412403623874</span></span><br><span class="line"><span class="number">0.0</span></span><br><span class="line"><span class="comment">#这种方法非常低效,因此 SentencePiece 使用了没有标记 X 的模型损失的近似值:它不是从头开始,而是通过其在剩余词汇表中的分段替换标记 X。这样,所有分数可以与模型损失同时计算。</span></span><br></pre></td></tr></table></figure>

<p>最后将模型使用的特殊标记添加到词汇表中，然后循环，直到从词汇表中获得了了足够的标记，以达到想要的大小:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">percent_to_remove = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(model) &gt; <span class="number">100</span>:</span><br><span class="line">    scores = compute_scores(model)</span><br><span class="line">    sorted_scores = <span class="built_in">sorted</span>(scores.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># Remove percent_to_remove tokens with the lowest scores.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(<span class="built_in">len</span>(model) * percent_to_remove)):</span><br><span class="line">        _ = token_freqs.pop(sorted_scores[i][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    total_sum = <span class="built_in">sum</span>([freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs.items()])</span><br><span class="line">    model = &#123;token: -log(freq / total_sum) <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs.items()&#125;</span><br><span class="line"><span class="comment">#为了标记一些文本需要应用预标记化，使用encode_word()函数:</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text, model</span>):</span><br><span class="line">    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    pre_tokenized_text = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> words_with_offsets]</span><br><span class="line">    encoded_words = [encode_word(word, model)[<span class="number">0</span>] <span class="keyword">for</span> word <span class="keyword">in</span> pre_tokenized_text]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(encoded_words, [])</span><br><span class="line">tokenize(<span class="string">&quot;This is the Hugging Face course.&quot;</span>, model)</span><br><span class="line">Output:</span><br><span class="line">[<span class="string">&#x27;▁This&#x27;</span>, <span class="string">&#x27;▁is&#x27;</span>, <span class="string">&#x27;▁the&#x27;</span>, <span class="string">&#x27;▁Hugging&#x27;</span>, <span class="string">&#x27;▁Face&#x27;</span>, <span class="string">&#x27;▁&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;ou&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h1 id="逐块地构建标记器"><a href="#逐块地构建标记器" class="headerlink" title="逐块地构建标记器"></a>逐块地构建标记器</h1><blockquote>
<p>标记化包括几个步骤：</p>
<ul>
<li>规范化（任何认为必要的文本清理，例如删除空格或重音符号、Unicode 规范化等）</li>
<li>预标记化（将输入拆分为单词）</li>
<li>通过模型处理输入（使用预先拆分的词来生成一系列标记）</li>
<li>后处理（添加标记器的特殊标记，生成注意力掩码和标记类型 ID）</li>
</ul>
</blockquote>
<p><img src="C:\Users\a2693\AppData\Roaming\Typora\typora-user-images\image-20240428124703577.png" alt="image-20240428124703577"></p>
<blockquote>
<p>该库是围绕一个中央“Tokenizer”类构建的，构建这个类的每一部分可以在子模块的列表中重新组合：</p>
<ul>
<li><code>normalizers</code> 包含你可以使用的所有可能的Normalizer类型（完整列表<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/normalizers">在这里</a>）。</li>
<li><code>pre_tokenizesr</code> 包含您可以使用的所有可能的PreTokenizer类型（完整列表<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/pre-tokenizers">在这里</a>）。</li>
<li><code>models</code> 包含您可以使用的各种类型的Model，如BPE、WordPiece和Unigram（完整列表<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/models">在这里</a>）。</li>
<li><code>trainers</code> 包含所有不同类型的 trainer，你可以使用一个语料库训练你的模型（每种模型一个；完整列表<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/trainers">在这里</a>）。</li>
<li><code>post_processors</code> 包含你可以使用的各种类型的PostProcessor（完整列表<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/post-processors">在这里</a>）。</li>
<li><code>decoders</code> 包含各种类型的Decoder，可以用来解码标记化的输出（完整列表<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/components#decoders">在这里</a>）。</li>
</ul>
<p>您可以<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/components">在这里</a>找到完整的模块列表。</p>
</blockquote>
<h2 id="获取语料库"><a href="#获取语料库" class="headerlink" title="获取语料库"></a>获取语料库</h2><p>详细步骤在Chapter6，第二节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">dataset = load_dataset(<span class="string">&quot;wikitext&quot;</span>, name=<span class="string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_training_corpus</span>():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), <span class="number">1000</span>):</span><br><span class="line">        <span class="keyword">yield</span> dataset[i : i + <span class="number">1000</span>][<span class="string">&quot;text&quot;</span>]</span><br><span class="line"><span class="comment">#get_training_corpus() 函数是一个生成器，每次调用的时候将产生 1,000 个文本，我们将用它来训练标记器。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenizers 也可以直接在文本文件上进行训练。以下是我们如何生成一个文本文件，其中包含我们可以在本地使用的来自 WikiText-2 的所有文本/输入：</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;wikitext-2.txt&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)):</span><br><span class="line">        f.write(dataset[i][<span class="string">&quot;text&quot;</span>] + <span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="从头开始构建-Word-Piece-标记器"><a href="#从头开始构建-Word-Piece-标记器" class="headerlink" title="从头开始构建 Word Piece 标记器"></a>从头开始构建 Word Piece 标记器</h2><p>要使用Tokenizers 库构建标记器，首先使用<strong>model</strong>实例化一个 <strong>Tokenizer</strong> 对象，然后将 <strong>normalizer</strong> , <strong>pre_tokenizer</strong> , <strong>post_processor</strong> ， 和 <strong>decoder</strong> 属性设置成我们想要的值。<br>对于这个例子，我们将创建一个 <strong>Tokenizer</strong> 使用 Word Piece 模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> (</span><br><span class="line">    decoders,</span><br><span class="line">    models,</span><br><span class="line">    normalizers,</span><br><span class="line">    pre_tokenizers,</span><br><span class="line">    processors,</span><br><span class="line">    trainers,</span><br><span class="line">    Tokenizer,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(models.WordPiece(unk_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/HuggingFace%E5%AD%A6%E4%B9%A0/" rel="tag"># HuggingFace学习　</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/04/24/HuggingFace%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-%E7%AC%AC%E4%BA%94%E7%AB%A0/" rel="prev" title="HuggingFace学习记录-第五章">
      <i class="fa fa-chevron-left"></i> HuggingFace学习记录-第五章
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/04/24/HuggingFace%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95-%E7%AC%AC%E4%B8%83%E7%AB%A0/" rel="next" title="HuggingFace学习记录-第七章">
      HuggingFace学习记录-第七章 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E5%B7%B2%E6%9C%89%E7%9A%84%E5%88%86%E8%AF%8D%E5%99%A8%E8%AE%AD%E7%BB%83%E6%96%B0%E7%9A%84%E5%88%86%E8%AF%8D%E5%99%A8"><span class="nav-number">1.</span> <span class="nav-text">根据已有的分词器训练新的分词器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BF%AB%E9%80%9F%E6%A0%87%E8%AE%B0%E5%99%A8%E7%9A%84%E7%89%B9%E6%AE%8A%E8%83%BD%E5%8A%9B"><span class="nav-number">2.</span> <span class="nav-text">快速标记器的特殊能力</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#QA-%E7%AE%A1%E9%81%93%E4%B8%AD%E7%9A%84%E5%BF%AB%E9%80%9F%E6%A0%87%E8%AE%B0%E5%99%A8"><span class="nav-number">3.</span> <span class="nav-text">QA 管道中的快速标记器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E9%A2%84%E6%A0%87%E8%AE%B0%E5%8C%96"><span class="nav-number">4.</span> <span class="nav-text">标准化和预标记化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E6%A0%87%E8%AE%B0%E5%8C%96"><span class="nav-number">4.1.</span> <span class="nav-text">预标记化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%A5%E5%AD%90"><span class="nav-number">4.2.</span> <span class="nav-text">句子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AD%97%E8%8A%82%E5%AF%B9%E7%BC%96%E7%A0%81%E6%A0%87%E8%AE%B0%E5%8C%96BPE"><span class="nav-number">5.</span> <span class="nav-text">字节对编码标记化BPE</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%87%E8%AE%B0%E4%B8%8E%E5%90%88%E5%B9%B6"><span class="nav-number">5.1.</span> <span class="nav-text">标记与合并</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%87%E8%AE%B0%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">5.2.</span> <span class="nav-text">标记化算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-BPE"><span class="nav-number">5.3.</span> <span class="nav-text">实现 BPE</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Word-Piece"><span class="nav-number">6.</span> <span class="nav-text">Word Piece</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%87%E8%AE%B0%E5%8C%96%E7%AE%97%E6%B3%95-1"><span class="nav-number">6.1.</span> <span class="nav-text">标记化算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-Word-Piece"><span class="nav-number">6.2.</span> <span class="nav-text">实现 Word Piece</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Unigram"><span class="nav-number">7.</span> <span class="nav-text">Unigram</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%87%E8%AE%B0%E5%8C%96%E7%AE%97%E6%B3%95-2"><span class="nav-number">7.1.</span> <span class="nav-text">标记化算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">7.2.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0-Unigram"><span class="nav-number">7.3.</span> <span class="nav-text">实现 Unigram</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%80%90%E5%9D%97%E5%9C%B0%E6%9E%84%E5%BB%BA%E6%A0%87%E8%AE%B0%E5%99%A8"><span class="nav-number">8.</span> <span class="nav-text">逐块地构建标记器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96%E8%AF%AD%E6%96%99%E5%BA%93"><span class="nav-number">8.1.</span> <span class="nav-text">获取语料库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA-Word-Piece-%E6%A0%87%E8%AE%B0%E5%99%A8"><span class="nav-number">8.2.</span> <span class="nav-text">从头开始构建 Word Piece 标记器</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Gimlettt163</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Gimlettt163</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
